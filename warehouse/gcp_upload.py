import os
import requests
import pandas as pd
import functions_framework
from google.cloud import storage, bigquery
from io import BytesIO
import datetime

# Initialize clients without hardcoded keys
# Google will automatically use the Service Account attached to the function
storage_client = storage.Client()
bq_client = bigquery.Client()

BUCKET_NAME = "de-zoomcamp-484617-function_bucket"
BASE_URL = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{}-{}.parquet"
DATASET_ID = "de_dataset" # Replace with your dataset
TABLE_ID = "yellow_tripdata"
STAGING_TABLE = "yellow_tripdata_tmp"

def get_or_create_bucket(bucket_name):
    bucket = storage_client.bucket(bucket_name)
    if not bucket.exists():
        storage_client.create_bucket(bucket_name)
        print(f"Created bucket {bucket_name}")
    return bucket

def test_run():
    try:
        bucket = get_or_create_bucket(BUCKET_NAME)
        # 1. Download to memory
        response = requests.get("https://www.timestored.com/data/sample/userdata.parquet")
        response.raise_for_status()

        # 2. Add columns using Pandas
        # Using BytesIO to avoid writing to disk unless necessary
        df = pd.read_parquet(BytesIO(response.content))

        # 3. Upload raw/transformed parquet to GCS
        blob = bucket.blob(f"test.parquet")
        blob.upload_from_string(df.to_parquet(), content_type='application/octet-stream')
    except Exception as e:
        print(f"Error in test run: {e}")
        return {"status": "failed", "error": str(e)}
    return {"status": "success", "message": "Test run completed"}

@functions_framework.http
def run_data_pipeline(request):
    # Get month range from input JSON
    # Payload example: {"start": 1, "end": 2}
    request_json = request.get_json(silent=True) or {}
    if not request_json:
        print("Ready to recieve input; running test run")
        return test_run()

    try:
        year = int(request_json.get('year', 2019))
        start_month = int(request_json.get('start_month', 1))
        end_month = int(request_json.get('end_month', 1))
    except Exception as e:
        print("Error in input parameters")
        return {"status": "failed", "error": "Invalid input parameters"}
    
    bucket = get_or_create_bucket(BUCKET_NAME)
    results = []

    for m in range(start_month, end_month + 1):
        
        if year <= 2019 or year >= datetime.datetime.now().year - 1:
            print("Invalid year range")
            return {"status": "failed", "error": "Invalid year range"}
        if m < 1 or m > 12:
            print("Invalid month range")
            results.append(f"Month {m} failed: Out of range")
            continue
        
        year_str = f"{year}"
        month_str = f"{m:02d}"
        url = BASE_URL.format(year_str, month_str)
        
        try:
            print(f"--- Processing yellow_tripdata_{year_str}-{month_str}.parquet ---")
            # 1. Download raw parquet (stream directly)
            response = requests.get(url, stream = True)
            response.raise_for_status()
            
            # 2. Upload raw parquet to GCS
            blob = bucket.blob(f"yellow_tripdata_{year_str}-{month_str}.parquet")
            # blob.upload_from_string(response.raw, content_type='application/octet-stream')
            with blob.open("wb") as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            print(f"Streamed to gs://{BUCKET_NAME}/yellow_tripdata_{year_str}-{month_str}.parquet")
            
            # 3. Load GCS to Staging Table (Overwrite staging each time)
            uri = f"gs://{BUCKET_NAME}/yellow_tripdata_{year_str}-{month_str}.parquet"
            job_config = bigquery.LoadJobConfig(
                source_format = bigquery.SourceFormat.PARQUET,
                write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
            )

            load_job = bq_client.load_table_from_uri(
                uri,
                f"{bq_client.project}.{DATASET_ID}.{STAGING_TABLE}",
                job_config=job_config
            )
            load_job.result() # Wait for job to finish
            print(f"Loaded to {DATASET_ID}.{STAGING_TABLE}")
            
            # 4. Create final table if not exists
            query = f"""
            CREATE TABLE IF NOT EXISTS `{bq_client.project}.{DATASET_ID}.{TABLE_ID}`
            (
                unique_row_id BYTES OPTIONS (description = 'A unique identifier for the trip, generated by hashing key trip attributes.'),
                filename STRING OPTIONS (description = 'The source filename from which the trip data was loaded.'),      
                VendorID INT64 OPTIONS (description = 'A code indicating the LPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.'),
                tpep_pickup_datetime TIMESTAMP OPTIONS (description = 'The date and time when the meter was engaged'),
                tpep_dropoff_datetime TIMESTAMP OPTIONS (description = 'The date and time when the meter was disengaged'),
                passenger_count INTEGER OPTIONS (description = 'The number of passengers in the vehicle. This is a driver-entered value.'),
                trip_distance FLOAT64 OPTIONS (description = 'The elapsed trip distance in miles reported by the taximeter.'),
                RatecodeID INT64 OPTIONS (description = 'The final rate code in effect at the end of the trip. 1= Standard rate 2=JFK 3=Newark 4=Nassau or Westchester 5=Negotiated fare 6=Group ride'),
                store_and_fwd_flag STRING OPTIONS (description = 'This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka "store and forward," because the vehicle did not have a connection to the server. TRUE = store and forward trip, FALSE = not a store and forward trip'),
                PULocationID INT64 OPTIONS (description = 'TLC Taxi Zone in which the taximeter was engaged'),
                DOLocationID INT64 OPTIONS (description = 'TLC Taxi Zone in which the taximeter was disengaged'),
                payment_type INTEGER OPTIONS (description = 'A numeric code signifying how the passenger paid for the trip. 1= Credit card 2= Cash 3= No charge 4= Dispute 5= Unknown 6= Voided trip'),
                fare_amount FLOAT64 OPTIONS (description = 'The time-and-distance fare calculated by the meter'),
                extra FLOAT64 OPTIONS (description = 'Miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges'),
                mta_tax FLOAT64 OPTIONS (description = '$0.50 MTA tax that is automatically triggered based on the metered rate in use'),
                tip_amount FLOAT64 OPTIONS (description = 'Tip amount. This field is automatically populated for credit card tips. Cash tips are not included.'),
                tolls_amount FLOAT64 OPTIONS (description = 'Total amount of all tolls paid in trip.'),
                improvement_surcharge FLOAT64 OPTIONS (description = '$0.30 improvement surcharge assessed on hailed trips at the flag drop. The improvement surcharge began being levied in 2015.'),
                total_amount FLOAT64 OPTIONS (description = 'The total amount charged to passengers. Does not include cash tips.'),
                congestion_surcharge FLOAT64 OPTIONS (description = 'Congestion surcharge applied to trips in congested zones')
            )
            PARTITION BY DATE(tpep_pickup_datetime);
            """
            bq_client.query(query).result()
            print(f"{TABLE_ID} Should Exist Now and Ready to Recieve Data")

            # 5. MERGE from Staging to Final with Hashing
            merge_query = f"""
            MERGE `{bq_client.project}.{DATASET_ID}.{TABLE_ID}` T
            USING (
                SELECT 
                    MD5(CONCAT(
                        IFNULL(CAST(VendorID AS STRING), ""),
                        IFNULL(CAST(tpep_pickup_datetime AS STRING), ""),
                        IFNULL(CAST(tpep_dropoff_datetime AS STRING), ""),
                        IFNULL(CAST(PULocationID AS STRING), ""),
                        IFNULL(CAST(DOLocationID AS STRING), "")
                    )) AS unique_row_id,
                    'yellow_tripdata_{year_str}-{month_str}.parquet' AS filename,
                    *
                FROM `{bq_client.project}.{DATASET_ID}.{STAGING_TABLE}`
            ) S
            ON T.unique_row_id = S.unique_row_id
            WHEN NOT MATCHED THEN
                INSERT (unique_row_id, filename, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, congestion_surcharge)
                VALUES (S.unique_row_id, S.filename, S.VendorID, S.tpep_pickup_datetime, S.tpep_dropoff_datetime, S.passenger_count, S.trip_distance, S.RatecodeID, S.store_and_fwd_flag, S.PULocationID, S.DOLocationID, S.payment_type, S.fare_amount, S.extra, S.mta_tax, S.tip_amount, S.tolls_amount, S.improvement_surcharge, S.total_amount, S.congestion_surcharge);
            """
            bq_client.query(merge_query).result()
            print(f"Data Merged to {DATASET_ID}.{TABLE_ID}")
            
            results.append(f"Month {month_str} success")
            
        except Exception as e:
            print(f"Error processing month {month_str}: {e}")
            results.append(f"Month {month_str} failed: {e}")

    return {"status": "completed", "results": results}, 200